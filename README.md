# ğŸ¦€ CrabPath

**Activation-driven memory graphs for AI agents. Everything evolves into this.**

---

In biology, [carcinisation](https://en.wikipedia.org/wiki/Carcinisation) is the phenomenon where crustaceans independently evolve into crab-like forms â€” over and over again. Nature keeps reinventing the crab because the crab body plan *works*.

CrabPath is a bet that agent memory systems will undergo the same convergence. Every serious agent memory system will eventually arrive at: **a weighted directed graph where activation spreads based on relevance, connections learn from outcomes, and frequently-used paths compile into cached reflexes.**

This is that system.

## What CrabPath Does

Instead of loading the same pile of files every session (or doing naive vector search), CrabPath:

1. **Propagates activation** through a learned memory graph when a task arrives
2. **Loads only what lights up** â€” the handful of facts, procedures, and constraints that matter for *this* task
3. **Learns from outcomes** â€” successful paths strengthen, failed paths weaken, bad patterns get inhibited
4. **Compiles hot paths** â€” frequently-successful routes become cached reflexes that cost nearly nothing to run

The result: agent memory that gets faster, cheaper, and more accurate over time.

## Key Concepts

### The Graph

Nodes aren't just text chunks. They're typed:

| Type | What it represents | Example |
|------|-------------------|---------|
| **Fact** | Declarative knowledge | "prod uses env var X" |
| **Rule** | Behavioral constraint | "never claim fixed without testing" |
| **Tool** | Tool affordance + local best practice | "use pty:true for Codex" |
| **Action** | Atomic executable step | `git diff HEAD~1 -- config/` |
| **Sequence** | Multi-step playbook | "debug deploy: config â†’ logs â†’ rollback" |
| **Episode** | Interaction trace with outcome | session log + success/failure label |
| **Error Class** | Reusable failure pattern | "context bloat", "stale cache" |

Edges are typed too: **Association**, **Sequence** (then), **Causation**, **Contingency** (if/else), **Inhibition** (blocks), **Preference**, **Abstraction**.

### Three-Tier Compute

Like biological nervous systems, CrabPath routes queries through three layers:

| Layer | Mechanism | Cost | When |
|-------|-----------|------|------|
| **Reflex** | Cached compiled path | ~$0 | Routine tasks (target: 70%) |
| **Habitual** | Cheap model activation | ~$0.001/query | Pattern matching |
| **Deliberative** | Full model reasoning | ~$0.01-0.80/query | Novel tasks (target: 5%) |

As the graph learns, more queries drop from deliberative â†’ habitual â†’ reflex.

### Myelination (Compiled Reflexes)

When a path fires often enough with consistent success, CrabPath compiles it into a deterministic macro â€” like how practiced skills become automatic in the brain. These are formalized as **options** (semi-MDP macro-actions) with:

- Initiation conditions (when to trigger)
- Internal policy (what to do)  
- Termination conditions (when to stop)
- Verification checks (did it work?)
- Fallback (degrade gracefully if environment changed)

### Learning Rules

- **Hebbian strengthening**: co-activated nodes during success â†’ strengthen edges
- **Inhibition learning**: actions that caused failures â†’ strengthen blocking edges
- **Decay**: unused connections fade over time
- **Pruning**: dead nodes/edges get archived
- **Consolidation**: nightly replay merges duplicates, promotes patterns to sequences
- **Immune system**: quarantine nodes correlated with repeated failures

## Status

ğŸš§ **Early development** â€” we're building this in the open.

CrabPath grew out of running three persistent LLM agents for 20+ days ($13K+ in API costs, 1,482 classified interactions, 252 corrections). The paper documents the architecture, the math, and the experimental plan.

### What exists today
- [Research paper](https://jonathangu.com/crabpath/) with full architecture spec
- Empirical data from multi-agent operation (warm-start corpus)
- Learning harness with 252 classified corrections (130 behavioral gates, 122 factual refs)
- Eval suite with 16 golden tasks

### What we're building
- [ ] Core graph data structure (NetworkX + SQLite)
- [ ] Activation propagation engine
- [ ] Warm-start pipeline (bootstrap from existing agent logs)
- [ ] Hebbian learning loop with credit assignment
- [ ] Myelination compiler (option extraction)
- [ ] Offline replay evaluation framework
- [ ] Integration with [OpenClaw](https://github.com/openclaw/openclaw)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CrabPath                    â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Reflex   â”‚   â”‚ Habitual â”‚   â”‚Delibera-â”‚ â”‚
â”‚  â”‚  Cache    â”‚â”€â”€â–¶â”‚  Router  â”‚â”€â”€â–¶â”‚  tive    â”‚ â”‚
â”‚  â”‚  (free)   â”‚   â”‚ (cheap)  â”‚   â”‚ (smart) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â–²              â”‚              â”‚        â”‚
â”‚       â”‚              â–¼              â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Memory Graph (G)             â”‚   â”‚
â”‚  â”‚  nodes: facts, rules, tools, actions â”‚   â”‚
â”‚  â”‚  edges: sequence, inhibition, cause  â”‚   â”‚
â”‚  â”‚  weights: learned from outcomes      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â–²              â”‚                       â”‚
â”‚       â”‚              â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Learning â”‚   â”‚  Nightly â”‚                â”‚
â”‚  â”‚   Loop   â”‚   â”‚ Cleanup  â”‚                â”‚
â”‚  â”‚(Hebbian) â”‚   â”‚(consolid)â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                    â”‚
         â”‚    Agent Tasks     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   LLM Agent (OpenClaw etc)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## The Paper

ğŸ“„ **Read the full paper**: [jonathangu.com/crabpath](https://jonathangu.com/crabpath/)

The paper covers:
- Why static context loading fails and what to do about it
- The full CrabPath architecture (typed graph, activation dynamics, learning rules)
- Mathematical framework (spectral graph theory, PageRank connection, MDP formulation)
- 10 biological mechanisms as testable engineering hypotheses
- Cost analysis and myelination economics
- Warm-start pipeline from existing agent data
- Experimental plan with ablation matrix
- Honest accounting of limitations and failure modes

## Quick Start

### Install

```bash
pip install crabpath
```

### Bootstrap from an OpenClaw workspace

```bash
# Import your workspace files + learning harness into a CrabPath graph
crabpath import-openclaw ~/.openclaw/workspace/

# Check what you got
crabpath stats

# Query the graph
crabpath activate "deployment failed after config change" --json
```

### Use as a library

```python
from crabpath.graph import MemoryGraph, MemoryNode, MemoryEdge, NodeType, EdgeType
from crabpath.activation import ActivationEngine

# Create a graph
graph = MemoryGraph(db_path="my-agent.db")

# Add nodes
graph.add_node(MemoryNode(
    id="rule-1",
    node_type=NodeType.RULE,
    content="Never claim fixed without testing on prod",
    summary="test before claiming fixed",
    tags=["deploy", "verification"],
    prior=0.9,
))

# Bootstrap from OpenClaw
from crabpath.openclaw import import_workspace
from pathlib import Path
stats = import_workspace(graph, Path("~/.openclaw/workspace/").expanduser())

# Activate the graph for a query
engine = ActivationEngine(graph)
result = engine.activate("deployment failed after config change")

for node, score in result.activated_nodes:
    print(f"[{score:.3f}] {node.node_type.value}: {node.summary}")

# Learn from outcome
engine.learn(result, outcome="success")
```

## Why "CrabPath"?

ğŸ¦€ **Carcinisation** â€” nature's most repeated evolutionary experiment. Unrelated crustaceans keep independently evolving into crabs because the crab form is a local optimum that *works*.

We believe activation-driven memory graphs are the "crab form" of AI agent memory. Every system that gets serious about persistent agents will converge here: weighted graphs, spreading activation, inhibition, compiled reflexes.

CrabPath is the path everything walks. ğŸ¦€

## Contributing

We welcome contributions! This is early-stage research turning into a real system. If you're interested in:

- Graph algorithms and activation dynamics
- LLM agent memory systems
- Reinforcement learning / online learning
- Biological inspiration in AI systems
- Building developer tools

...come help us build the crab. ğŸ¦€

## License

Apache 2.0 â€” see [LICENSE](LICENSE).

## Citation

```bibtex
@misc{gu2026crabpath,
  title={CrabPath: Activation-Driven Memory Graphs for AI Agents},
  author={Gu, Jonathan},
  year={2026},
  url={https://github.com/jonathangu/crabpath}
}
```

---

*Built by [Jonathan Gu](https://jonathangu.com) â€¢ Powered by too many late nights and $13K in API costs* ğŸ¦€
